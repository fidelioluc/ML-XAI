{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-08T16:26:42.811189Z",
     "start_time": "2025-06-08T16:26:42.806352Z"
    }
   },
   "source": [
    "from vectorize import apply_tfidf, apply_doc2vec, apply_sbert\n",
    "import pandas as pd\n",
    "import sys\n",
    "print(sys.executable)\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk import word_tokenize"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fidel\\miniconda3\\envs\\ML-XAI\\python.exe\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T16:36:33.165705Z",
     "start_time": "2025-06-08T16:36:33.040775Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv(\"../data/20newsgroup_preprocessed.csv\")",
   "id": "5db6096f409a4edc",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T16:43:09.132632Z",
     "start_time": "2025-06-08T16:43:08.809371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_tfidf, tfidf_vectorizer = apply_tfidf(df['text'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(\n",
    "    X_tfidf.toarray(),\n",
    "    columns=tfidf_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "print(tfidf_df.head())"
   ],
   "id": "5e8ae64fd6c5f932",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m X_tfidf, tfidf_vectorizer = apply_tfidf(df[\u001B[33m'\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m'\u001B[39m])\n\u001B[32m      3\u001B[39m tfidf_df = pd.DataFrame(\n\u001B[32m      4\u001B[39m     X_tfidf.toarray(),\n\u001B[32m      5\u001B[39m     columns=tfidf_vectorizer.get_feature_names_out()\n\u001B[32m      6\u001B[39m )\n\u001B[32m      8\u001B[39m \u001B[38;5;28mprint\u001B[39m(tfidf_df.head())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ML-XAI\\vectorization\\vectorize.py:9\u001B[39m, in \u001B[36mapply_tfidf\u001B[39m\u001B[34m(docs, max_features)\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mapply_tfidf\u001B[39m(docs, max_features=\u001B[32m1000\u001B[39m):\n\u001B[32m      8\u001B[39m     vectorizer = TfidfVectorizer(lowercase=\u001B[38;5;28;01mTrue\u001B[39;00m, stop_words=\u001B[33m'\u001B[39m\u001B[33menglish\u001B[39m\u001B[33m'\u001B[39m, max_features=max_features)\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m     X = vectorizer.fit_transform(docs)\n\u001B[32m     10\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m X, vectorizer\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\ML-XAI\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001B[39m, in \u001B[36mTfidfVectorizer.fit_transform\u001B[39m\u001B[34m(self, raw_documents, y)\u001B[39m\n\u001B[32m   2097\u001B[39m \u001B[38;5;28mself\u001B[39m._check_params()\n\u001B[32m   2098\u001B[39m \u001B[38;5;28mself\u001B[39m._tfidf = TfidfTransformer(\n\u001B[32m   2099\u001B[39m     norm=\u001B[38;5;28mself\u001B[39m.norm,\n\u001B[32m   2100\u001B[39m     use_idf=\u001B[38;5;28mself\u001B[39m.use_idf,\n\u001B[32m   2101\u001B[39m     smooth_idf=\u001B[38;5;28mself\u001B[39m.smooth_idf,\n\u001B[32m   2102\u001B[39m     sublinear_tf=\u001B[38;5;28mself\u001B[39m.sublinear_tf,\n\u001B[32m   2103\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m2104\u001B[39m X = \u001B[38;5;28msuper\u001B[39m().fit_transform(raw_documents)\n\u001B[32m   2105\u001B[39m \u001B[38;5;28mself\u001B[39m._tfidf.fit(X)\n\u001B[32m   2106\u001B[39m \u001B[38;5;66;03m# X is already a transformed view of raw_documents so\u001B[39;00m\n\u001B[32m   2107\u001B[39m \u001B[38;5;66;03m# we set copy to False\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\ML-XAI\\Lib\\site-packages\\sklearn\\base.py:1389\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1382\u001B[39m     estimator._validate_params()\n\u001B[32m   1384\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1385\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1386\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1387\u001B[39m     )\n\u001B[32m   1388\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\ML-XAI\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001B[39m, in \u001B[36mCountVectorizer.fit_transform\u001B[39m\u001B[34m(self, raw_documents, y)\u001B[39m\n\u001B[32m   1368\u001B[39m             warnings.warn(\n\u001B[32m   1369\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mUpper case characters found in\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1370\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33m vocabulary while \u001B[39m\u001B[33m'\u001B[39m\u001B[33mlowercase\u001B[39m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1371\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33m is True. These entries will not\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1372\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33m be matched with any documents\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1373\u001B[39m             )\n\u001B[32m   1374\u001B[39m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1376\u001B[39m vocabulary, X = \u001B[38;5;28mself\u001B[39m._count_vocab(raw_documents, \u001B[38;5;28mself\u001B[39m.fixed_vocabulary_)\n\u001B[32m   1378\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.binary:\n\u001B[32m   1379\u001B[39m     X.data.fill(\u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\ML-XAI\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001B[39m, in \u001B[36mCountVectorizer._count_vocab\u001B[39m\u001B[34m(self, raw_documents, fixed_vocab)\u001B[39m\n\u001B[32m   1261\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m raw_documents:\n\u001B[32m   1262\u001B[39m     feature_counter = {}\n\u001B[32m-> \u001B[39m\u001B[32m1263\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m analyze(doc):\n\u001B[32m   1264\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1265\u001B[39m             feature_idx = vocabulary[feature]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\ML-XAI\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:99\u001B[39m, in \u001B[36m_analyze\u001B[39m\u001B[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[39m\n\u001B[32m     77\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Chain together an optional series of text processing steps to go from\u001B[39;00m\n\u001B[32m     78\u001B[39m \u001B[33;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001B[39;00m\n\u001B[32m     79\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m     95\u001B[39m \u001B[33;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001B[39;00m\n\u001B[32m     96\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     98\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m decoder \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m99\u001B[39m     doc = decoder(doc)\n\u001B[32m    100\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m analyzer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    101\u001B[39m     doc = analyzer(doc)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\ML-XAI\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:232\u001B[39m, in \u001B[36m_VectorizerMixin.decode\u001B[39m\u001B[34m(self, doc)\u001B[39m\n\u001B[32m    229\u001B[39m     doc = doc.decode(\u001B[38;5;28mself\u001B[39m.encoding, \u001B[38;5;28mself\u001B[39m.decode_error)\n\u001B[32m    231\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m doc \u001B[38;5;129;01mis\u001B[39;00m np.nan:\n\u001B[32m--> \u001B[39m\u001B[32m232\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    233\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mnp.nan is an invalid document, expected byte or unicode string.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    234\u001B[39m     )\n\u001B[32m    236\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m doc\n",
      "\u001B[31mValueError\u001B[39m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
